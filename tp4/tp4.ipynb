{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4 : travail final sur l'évolution de l'image des femmes scientifiques francophones dans le corpus CAMille"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constitution des critères du sous-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import des librairies\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL de la page Wikipedia\n",
    "url = \"https://fr.wikipedia.org/wiki/Liste_de_femmes_scientifiques\"\n",
    "\n",
    "# Téléchargement du contenu de la page\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sélection des scientifiques francophones\n",
    "# A partir des drapeaux Belgique, France et Suisse\n",
    "target_flags = [\n",
    "    'Flag_of_Belgium.svg',\n",
    "    'Flag_of_France.svg',\n",
    "    'Flag_of_Switzerland.svg'\n",
    "]\n",
    "\n",
    "# Sélectionner les éléments <li> correspondant aux pays cibles\n",
    "francophones = []\n",
    "for li in soup.find_all('li'):\n",
    "    flag_span = li.find('span', class_='flagicon')\n",
    "    if flag_span and any(flag in flag_span.a['href'] for flag in target_flags):\n",
    "        francophones.append(li)\n",
    "\n",
    "# Afficher les éléments <li> correspondants\n",
    "print(francophones[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sélection des francophones nées avant 1945\n",
    "scientists = []\n",
    "for li in francophones:\n",
    "    link = li.find('a', href=True)\n",
    "    if link:\n",
    "        # Trouver le texte contenant la date de naissance\n",
    "        text_parts = li.get_text().split('(')\n",
    "        if len(text_parts) > 1:\n",
    "            birth_year_text = text_parts[-1].split('-')[0]\n",
    "            if birth_year_text.isdigit():\n",
    "                birth_year = int(birth_year_text)\n",
    "                if birth_year < 1945:\n",
    "                    scientists.append(li)\n",
    "\n",
    "# Contrôler les éléments <li> correspondants\n",
    "for li_element in scientists:\n",
    "    print(li_element.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage du nombre de scientifiques identifiées\n",
    "len(scientists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nettoyer la liste des scientifiques\n",
    "for li in scientists:\n",
    "    # Trouver toutes les balises 'a' à l'intérieur de chaque balise 'li'\n",
    "    ancres = li.find_all('a', class_=False)\n",
    "    for ancre in ancres :\n",
    "        # Imprimer les titres à l'intérieur de chaque balise 'a'\n",
    "        scientist_name = ancre.text\n",
    "        # Ecarter les mots spécialités notées en minuscules\n",
    "        if not scientist_name.islower() :\n",
    "            print(scientist_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports et données pour le sous-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des librairies\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences\n",
    "\n",
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# répertoires\n",
    "indir = \"../data/tp4\" # import txt de Camille\n",
    "outdir = \"../data/tp4/txt_clean\" # fichiers txt nettoyés\n",
    "\n",
    "# données\n",
    "annee1 = 1870 # première année d'apparition\n",
    "all_years = [str(year) for year in range(int(annee1), 1971)]\n",
    "autrices =  {\"Marie Curie\", \"Irène Joliot-Curie\", \"Sophie Germain\", \"Lucia de Brouckère\"}\n",
    "infile = \"articles.txt\" #tout le sous-corpus dans 1 seul fichier\n",
    "outfile = \"../data/tp4/sents.txt\" #analyse de sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lister les fichiers des articles \n",
    "files = []\n",
    "for f in os.listdir(indir):\n",
    "    if os.path.isfile(os.path.join(indir, f)):\n",
    "        files.append(f)\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter le nombre de fichiers\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorer le sous-corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### statistiques des journaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter le nombre de journaux représentés\n",
    "count_decade = defaultdict(int)\n",
    "count_month = defaultdict(int)\n",
    "count_newspapers = defaultdict(int)\n",
    "covered_years = set()\n",
    "\n",
    "for f in files:\n",
    "    if \"_\" in f and f.endswith(\"txt\"):\n",
    "        elems = f.split(\"_\")\n",
    "        if len(elems) < 3:\n",
    "            print(f\"Anomalous file: {f}\")\n",
    "            continue\n",
    "        newspaper = elems[1]\n",
    "\n",
    "        year = elems[2].split(\"-\")[0]\n",
    "        covered_years.add(year)\n",
    "        decade = year[:3] + \"0s\"\n",
    "        month = int(elems[2].split(\"-\")[1])\n",
    "        \n",
    "        count_decade[decade] += 1\n",
    "        count_newspapers[newspaper] += 1\n",
    "        count_month[month] += 1\n",
    "    else:\n",
    "        print(f\"Anomalous file: {f}\")\n",
    "\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cette sélection contient :')\n",
    "print(f\"{count_newspapers['JB421']} exemplaires de L'Avenir du Luxembourg\")\n",
    "print(f\"{count_newspapers['JB427']} exemplaires de La Libre Belgique\")\n",
    "print(f\"{count_newspapers['JB555']} exemplaires de L'Indépendance belge\")\n",
    "print(f\"{count_newspapers['JB555A']} exemplaires de L'Indépendance belge (édité en Angleterre)\")\n",
    "print(f\"{count_newspapers['JB567']} exemplaires du Journal de Bruxelles\")\n",
    "print(f\"{count_newspapers['JB572']} exemplaires du Journal de Charleroi\")\n",
    "print(f\"{count_newspapers['JB638']} exemplaires de La Meuse\")\n",
    "print(f\"{count_newspapers['JB685']} exemplaires de Le Petit Bleu\")\n",
    "print(f\"{count_newspapers['JB729']} exemplaires de Le Vingtième Siècle\")\n",
    "print(f\"{count_newspapers['JB773']} exemplaires de Vers l'Avenir\")\n",
    "print(f\"{count_newspapers['JB837']} exemplaires de Le Peuple\")\n",
    "print(f\"{count_newspapers['JB838']} exemplaires du journal Le Soir\")\n",
    "print(f\"{count_newspapers['JB1051']} exemplaires de Le Drapeau rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"La répartition par décennie :\")\n",
    "print(f\"{count_decade['1880s']} fichiers pour la décennie 1880s\")\n",
    "print(f\"{count_decade['1890s']} fichiers pour la décennie 1890s\")\n",
    "print(f\"{count_decade['1900s']} fichiers pour la décennie 1900s\")\n",
    "print(f\"{count_decade['1910s']} fichiers pour la décennie 1910s\")\n",
    "print(f\"{count_decade['1920s']} fichiers pour la décennie 1920s\")\n",
    "print(f\"{count_decade['1930s']} fichiers pour la décennie 1930s\")\n",
    "print(f\"{count_decade['1940s']} fichiers pour la décennie 1940s\")\n",
    "print(f\"{count_decade['1950s']} fichiers pour la décennie 1950s\")\n",
    "print(f\"{count_decade['1960s']} fichiers pour la décennie 1960s\")\n",
    "print(f\"{count_decade['1970s']} fichiers pour la décennie 1970s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_years = [y for y in all_years if y not in covered_years]\n",
    "print(f\"Années manquantes: {', '.join(missing_years)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nombre de citations par autrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste des citations par autrice par année\n",
    "autrices_annees ={}\n",
    "\n",
    "for autrice in autrices:\n",
    "    autrices_annees[autrice]={}\n",
    "    for annee in all_years:\n",
    "        autrices_annees[autrice][annee] = {}\n",
    "autrices_annees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comptage du nombre de citations par autrice par année\n",
    "nb_articles = 0\n",
    "total_articles = 0\n",
    "no_annee = annee\n",
    "for autrice in autrices :\n",
    "    total_articles += nb_articles\n",
    "    nb_articles = 0\n",
    "    #print(autrice)\n",
    "    for f in files:\n",
    "        if \"_\" in f and f.endswith(\"txt\") :\n",
    "            #print(f)\n",
    "            filepath = indir +\"/\" + f\n",
    "            file = open(filepath,\"r\", encoding=\"utf-8\")\n",
    "            lines = file.readlines()\n",
    "            elems = f.split(\"_\")\n",
    "            if len(elems) < 3:\n",
    "                #print(f\"Anomalous file: {f}\")\n",
    "                continue\n",
    "            annee = elems[2].split(\"-\")[0]\n",
    "            #print(annee)\n",
    "            for line in lines :\n",
    "                if autrice in line:\n",
    "                    #print(line[:100])\n",
    "                    if annee>no_annee :\n",
    "                        no_annee = annee\n",
    "                    nb_articles +=1\n",
    "                    #print(nb_articles)\n",
    "                    autrices_annees[autrice][annee] = nb_articles\n",
    "#print(total_articles)                    \n",
    "#autrices_annees\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autrices_annees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste pour formater les données\n",
    "data = []\n",
    "\n",
    "# Convertir les données en DataFrame pandas\n",
    "for personne, donnees_annees in autrices_annees.items():\n",
    "        for annee, nb_articles in donnees_annees.items():\n",
    "                data.append({\n",
    "                'Scientifiques' : personne,\n",
    "                'Année' : annee,\n",
    "                'Nombre d\\'articles' : nb_articles\n",
    "                })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un DataFrame pour pandas\n",
    "df = pd.DataFrame(autrices_annees)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corriger le type des données\n",
    "for autrice in autrices :\n",
    "    #df_sorted.fillna(autrice)\n",
    "    df[autrice]=pd.to_numeric(df[autrice])\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création du graphique\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.heatmap(df, cmap='RdYlGn_r', linewidths=0.5, annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyser le sous-corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparer le sous-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nettoyer le sous-corpus\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "\n",
    "#paramétrage des termes de recherche\n",
    "query = [\"Marie Curie\",\"Irène Joliot-Curie\",\"Sophie Germain\",\"Lucia de Brouckère\"]\n",
    "\n",
    "# Création d'une regex afin de trouver les mots de la liste query dans le texte\n",
    "regex = re.compile(f\"\\\\b({'|'.join(query)})\\\\b\", re.IGNORECASE)\n",
    "\n",
    "#nettoyage et export dans le nouveau répertoire\n",
    "for file in files:\n",
    "    if file.endswith(\".txt\"):\n",
    "        relevant_sentences = []\n",
    "        f_in = open(os.path.join(indir, file), encoding=\"utf-8\")\n",
    "        text = f_in.read()\n",
    "        for sentence in sent_tokenize(text):\n",
    "            if regex.search(sentence):\n",
    "                relevant_sentences.append(sentence)\n",
    "        f_in.close()\n",
    "        f_out = open(os.path.join(outdir, file), \"w\", encoding=\"utf-8\")\n",
    "        f_out.write(\"\\n\\n\".join(relevant_sentences))\n",
    "        f_out.close()\n",
    "\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lister les fichiers des articles \n",
    "files = []\n",
    "for f in os.listdir(outdir):\n",
    "    if os.path.isfile(os.path.join(indir, f)):\n",
    "        files.append(f)\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intégrer tous les fichiers dans un seul fichier texte\n",
    "articles = \"\"\n",
    "\n",
    "for f in files :\n",
    "    if \"_\" in f and f.endswith(\"txt\") :\n",
    "            filepath = outdir +\"/\" + f\n",
    "            file = open(filepath,\"r\", encoding=\"utf-8\")\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                #print(line)\n",
    "                articles += line\n",
    "print(articles[:100])\n",
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_export = open(infile, \"w\",encoding=\"utf-8\")\n",
    "f_export.write(articles)\n",
    "f_export.close\n",
    "\n",
    "f_export = open(infile, \"r\",encoding=\"utf-8\")\n",
    "print(f_export.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entités nommées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#charger le texte\n",
    "text = open(infile, encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Traiter le texte\n",
    "#nlp.max_length=len(articles)\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les entités\n",
    "people = defaultdict(int)\n",
    "place = defaultdict(int)\n",
    "organization = defaultdict(int)\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PER\" and len(ent.text) > 3:\n",
    "        people[ent.text] += 1\n",
    "    else : \n",
    "        if ent.label_ == \"LOC\" and len(ent.text) > 3:\n",
    "            place[ent.text] += 1\n",
    "        else : \n",
    "            if ent.label_ == \"ORG\" and len(ent.text) > 3:\n",
    "                organization[ent.text] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trier et imprimer\n",
    "#people\n",
    "sorted_people = sorted(people.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(\"Apparition des personnes dans le corpus\")\n",
    "for person, freq in sorted_people[:10]:\n",
    "    print(f\"{person} :{freq}\")\n",
    "#place\n",
    "print(\"---\")\n",
    "sorted_place = sorted(place.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(\"Apparition des localisations dans le corpus\")\n",
    "for place, freq in sorted_place[:10]:\n",
    "    print(f\"{place} : {freq}\")\n",
    "#organization\n",
    "print(\"---\")\n",
    "print(\"Apparition des organisations dans le corpus\")\n",
    "sorted_organization = sorted(organization.items(), key=lambda kv: kv[1], reverse=True)\n",
    "for organization, freq in sorted_organization[:10]:\n",
    "    print(f\"{organization} : {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Préparation du sous-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# créer un modèle de dictionnaire\n",
    "autrices_sentiments ={}\n",
    "\n",
    "for autrice in autrices:\n",
    "    autrices_sentiments[autrice]={}\n",
    "    for annee in all_years:\n",
    "        autrices_sentiments[autrice][annee] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autrices_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copier le modèle pour l'analyse des sentiments\n",
    "autrices_analyse = autrices_sentiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remplissage des citations dans autrices_sentiments\n",
    "\n",
    "liste_phrases=[]\n",
    "for autrice in autrices :\n",
    "    \n",
    "    #print(autrice)\n",
    "    for f in files:\n",
    "        if \"_\" in f and f.endswith(\"txt\") :\n",
    "            filepath = outdir +\"/\" + f\n",
    "            file = open(filepath,\"r\", encoding=\"utf-8\")\n",
    "            lines = file.readlines()\n",
    "            elems = f.split(\"_\")\n",
    "            if len(elems) < 3:\n",
    "                #print(f\"Anomalous file: {f}\")\n",
    "                continue\n",
    "            annee = elems[2].split(\"-\")[0]\n",
    "            citations = \"\"\n",
    "            #print(citations)\n",
    "            for line in lines :              \n",
    "                #print(citations)\n",
    "                if autrice in line:\n",
    "                    print(f)\n",
    "                    print(autrice)\n",
    "                    print(annee)\n",
    "                    print(line[:100])\n",
    "                    liste_phrases.append(line)\n",
    "                    autrices_sentiments[autrice][annee] = liste_phrases                                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autrices_sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "#from transformers import pipeline\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, BertForSequenceClassification\n",
    "from transformers import (XLMRobertaConfig, XLMRobertaTokenizer, TFXLMRobertaModel)            \n",
    "from transformers import AutoTokenizer, AutoConfig, TFAutoModel, TFAutoModelForSequenceClassification \n",
    "from transformers import pipeline\n",
    "\n",
    "PRETRAINED_MODEL_TYPES = {\n",
    "    'xlmroberta': (AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, \"tblard/tf-allocine\")\n",
    "}\n",
    "\n",
    "config_class, model_class, tokenizer_class, model_name = PRETRAINED_MODEL_TYPES['xlmroberta']\n",
    "\n",
    "# Download vocabulary from huggingface.co and cache.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tblard/tf-allocine\",use_fast=False) #fast tokenizer\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"tblard/tf-allocine\")\n",
    "\n",
    "sentiment_analyser = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyser le sentiment des phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse de quelques années choisies\n",
    "phrase1=autrices_sentiments['Marie Curie']['1934']\n",
    "print(len(phrase1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti1=sentiment_analyser(phrase1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "yes = 0\n",
    "for senti in senti1:\n",
    "    if 'POSITIVE' in senti :\n",
    "        nb_positif =+ 1\n",
    "moy_positif=nb_positif/len(senti1)\n",
    "print(moyenne_positif) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse des phrases et stockage dans un dictionnaire\n",
    "data_analyser = []\n",
    "autrices={'Marie Curie'}\n",
    "all_years2 = [str(year) for year in range(1933, 1935)]\n",
    "\n",
    "for autrice in autrices:\n",
    "    for annee in all_years:\n",
    "        phrases = autrices_sentiments[autrice][annee]\n",
    "        for phrase in phrases :\n",
    "            #print(phrase)\n",
    "            #print(len(phrase))\n",
    "            liste_analyser = []\n",
    "            if len(phrase) > 0 :\n",
    "                try :\n",
    "                    sentiment_analyser(phrase)\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur lors de l'analyse de la phrase : {phrase}\")\n",
    "                    autrices_sentiments[autrice][annee]=\"\" # si non analysable, phrase écartée du sous-corpus \n",
    "                    continue \n",
    "                phrase = autrices_sentiments[autrice][annee]\n",
    "                if len(phrase) > 2 :\n",
    "                    senti = sentiment_analyser(phrase)\n",
    "                    liste_analyser = [autrice, annee, senti]\n",
    "                    #print(liste_analyser)\n",
    "                    data_analyser.append(liste_analyser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire pour stocker les moyennes\n",
    "moyennes = {}\n",
    "\n",
    "for entry in data:\n",
    "    autrice, annee, liste_sentiments = entry\n",
    "    \n",
    "    if autrice not in moyennes:\n",
    "        moyennes[autrice] = {}\n",
    "    \n",
    "    if annee not in moyennes[autrice]:\n",
    "        moyennes[autrice][annee] = {'nb_avis': 0, 'nb_neutralite': 0, 'total_avis': 0, 'total_neutralite': 0}\n",
    "\n",
    "    #print(liste_sentiments)\n",
    "    list_senti = liste_sentiments\n",
    "    if isinstance(liste_sentiments, list) :\n",
    "        liste_senti = liste_sentiments\n",
    "        #print(liste_senti[:10])    \n",
    "    #dict_sentiments = sentiments[0]\n",
    "    #nb_sentiments = len(liste_sentiments)\n",
    "    #print(nb_sentiments)\n",
    "        for sentiment in list_senti:\n",
    "            avis = sentiment['label']\n",
    "            #print(avis)\n",
    "            neutralite = sentiment['score']\n",
    "        \n",
    "        # Mettre à jour les totaux\n",
    "            if avis == 'POSITIVE' or avis == 'NEGATIVE':  # Ignorer si le sentiment n'est pas positif ou négatif\n",
    "                moyennes[autrice][annee]['total_avis'] += 1\n",
    "                if avis == 'POSITIVE':\n",
    "                    moyennes[autrice][annee]['nb_avis'] += 1  \n",
    "                else :\n",
    "                    moyennes[autrice][annee]['nb_avis'] +=0\n",
    "        \n",
    "        moyennes[autrice][annee]['total_neutralite'] += neutralite\n",
    "        moyennes[autrice][annee]['nb_neutralite'] += 1\n",
    "    else :\n",
    "        if isinstance(liste_sentiments,int) :\n",
    "            #liste_senti = str(liste_sentiments)\n",
    "            #print(liste_senti[:10])\n",
    "            continue\n",
    "        else :\n",
    "            print('list_senti chaisplusquoifaire')\n",
    "\n",
    "# Calcul des moyennes\n",
    "for autrice, annees in moyennes.items():\n",
    "    for annee, valeurs in annees.items():\n",
    "        moyennes[autrice][annee]['moyenne_avis'] = valeurs['nb_avis'] / valeurs['total_avis'] if valeurs['total_avis'] > 0 else 0\n",
    "        moyennes[autrice][annee]['moyenne_neutralite'] = valeurs['total_neutralite'] / valeurs['nb_neutralite'] if valeurs['nb_neutralite'] > 0 else 0\n",
    "\n",
    "# Affichage des moyennes\n",
    "for autrice, annees in moyennes.items():\n",
    "    for annee, valeurs in annees.items():\n",
    "        print(f\"{autrice} {annee}: Moyenne d'avis - {valeurs['moyenne_avis']}, Moyenne de neutralite - {valeurs['moyenne_neutralite']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moyennes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moyennes['Irène Joliot-Curie']['1934'][\"moyenne_avis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### représentation graphique de l'évolution des avis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste pour formater les données des avis\n",
    "data_avis = []\n",
    "\n",
    "\n",
    "# Convertir les données en DataFrame pandas\n",
    "for autrice, annees in moyennes.items():\n",
    "    for annee, valeurs in annees.items():\n",
    "            data_avis.append({\n",
    "                'Scientifiques' : autrice,\n",
    "                'Année' : annee,\n",
    "                'Moyenne des avis' : valeurs['moyenne_avis']\n",
    "                 })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_avis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un DataFrame pour pandas\n",
    "df = pd.DataFrame(data_avis)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corriger le type des données\n",
    "for autrice in autrices :\n",
    "    #df_sorted.fillna(autrice)\n",
    "    df[autrice]=pd.to_numeric(df[autrice])\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corriger le type des données\n",
    "for autrice in autrices :\n",
    "    df_data_avis[autrice]=pd.to_numeric(df[autrice])\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data_avis.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création du graphique\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.heatmap(df_data_avis, cmap='RdYlGn_r', linewidths=0.5, annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_avis.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
